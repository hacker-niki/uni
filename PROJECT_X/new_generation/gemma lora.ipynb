{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ae7d614-9244-4280-ac3d-ebc28a00b1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U accelerate transformers peft trl datasets sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a1db84a-3100-4890-b0a9-8996d38a6a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a29c9425d10d40d78f773ceadff00f78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "hf_token = \"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\"\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e92889b-35a3-4f45-afc1-d1e2692d4f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e55352af9974649a33d996fbedeb302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/34.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9244fd7d55a64712a7f047e97db0e4c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c1fcf6fde1141008e52a800cbd2e2a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a01b286fb9e744b39ad691baf0f6bd26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dcf1e0f2a7d4e7daa104f414ef3869b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88fafc51ff7e4a6a9279edddbd8733fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dbef5ae7bc8454490d5d2c4988273ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c83ca8eb7fa64341b8ec0e3169398dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8751d1878dda4415ae6baf90e599475b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e4a30c16f334af681cd7aebaf2ed087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc2dacbbd214ebbba4e57d808de2629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель и токенайзер загружены на CPU.\n",
      "Устройство модели: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Настоятельно рекомендуется использовать 2B модель для CPU\n",
    "model_id = \"google/gemma-2b-it\"\n",
    "\n",
    "# Загрузка токенайзера\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side='right')\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Загрузка модели в полной точности (float32)\n",
    "# ВНИМАНИЕ: это потребует ~8-10 ГБ RAM\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float32, # Указываем стандартный тип данных для CPU\n",
    "    # Убираем все GPU-специфичные параметры:\n",
    "    # quantization_config, device_map, attn_implementation\n",
    ")\n",
    "\n",
    "print(\"Модель и токенайзер загружены на CPU.\")\n",
    "# Убедимся, что модель на CPU\n",
    "print(f\"Устройство модели: {model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb290b4e-ea47-432d-8dab-4735f9742428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный размер датасета: 45328\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fabb4b72e764a37b1b7ae9934bf9c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/45328 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7439d428ee244ada4bff6e3843212ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/45328 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер обучающего датасета: 40795\n",
      "Размер валидационного датасета: 4533\n",
      "\n",
      "Пример подготовленных данных (декодированный):\n",
      "<bos>Пещеры, образованные в результате действия поверхностных вод называют эрозионными (в отличии от карстовых пещер, образованных подземными водами), пещеры, образованные волнами морей и океанов в прибрежных скалах называют абразионными, а пещеры, образованные несущими песок ветрами на пустынных скалах называют коррозионными. Пещеры, образуемые в нерастворимых породах за счёт механической эрозии, то есть проработанные водой, содержащей крупинки твёрдого материала. Часто такие пещеры образуются на берегу моря под действием прибоя (Морская пещера), но они невелики. В пустынях под действием несущего песок ветра иногда образуются эоловые пещеры и эоловые гроты. Однако, возможно образование и пещер, проработанных по первичным тектоническим трещинам уходящими под землю ручьями. Известны довольно крупные (сотни метров длиной) эрозионные пещеры, образованные в песчаниках и даже гранитах. Примерами крупных эрозионных пещер могут быть T.S.O.D. (Touchy Sword of Damocles) Cave в габбро (4 км/−51 м, Нью-Йорк)[8], Bat Cave в гнейсах (1,7 км, Северная Каролина), Upper Millerton Lake Cave в гранитах (Калифорния)[9][10].\n",
      "{\"questions\": [{\"question\": \"Что иногда образуется в пустынях под действием несущего песок ветра?\", \"answers\": [{\"text\": \"Примерами крупных эрозионных пещер могут быть T\", \"is_correct\": false}, {\"text\": \"Однако, возможно образование и пещер, проработанных по первичным тектоническим трещинам уходящими под землю ручьями\", \"is_correct\": false}, {\"text\": \"Пещеры, образованные в результате действия поверхностных вод называют эрозионными (в отличии от карстовых пещер, образованных подземными водами), пещеры, образованные волнами морей и океанов в прибрежных скалах называют абразионными, а\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "dataset = load_dataset(\"sberquad\", split='train')\n",
    "print(f\"Исходный размер датасета: {len(dataset)}\")\n",
    "\n",
    "def generate_distractors(context, correct_answer):\n",
    "    \"\"\"Генерирует отвлекающие ответы из контекста.\"\"\"\n",
    "    sentences = context.split('.')\n",
    "    distractors = [s.strip() for s in sentences if correct_answer not in s and len(s.strip()) > 10]\n",
    "    while len(distractors) < 4:\n",
    "        distractors.append(random.choice([\n",
    "            \"Этот вариант заведомо неверный.\", \"Неправильная информация.\"\n",
    "        ]))\n",
    "    random.shuffle(distractors)\n",
    "    return list(set(distractors))[:4]\n",
    "\n",
    "def format_direct_mapping(sample):\n",
    "    context = sample['context']\n",
    "    question_text = sample['question']\n",
    "    correct_answer_text = sample['answers']['text'][0]\n",
    "    distractors = generate_distractors(context, correct_answer_text)\n",
    "    answers = [{\"text\": correct_answer_text, \"is_correct\": True}]\n",
    "    for d in distractors:\n",
    "        if d != correct_answer_text:\n",
    "            answers.append({\"text\": d, \"is_correct\": False})\n",
    "    random.shuffle(answers)\n",
    "    json_output = {\"question\": question_text, \"answers\": answers}\n",
    "    json_string = json.dumps(json_output, ensure_ascii=False)\n",
    "    full_text = f\"{context}\\n{json_string}\"\n",
    "    max_length_tokens = 512\n",
    "    result = tokenizer(\n",
    "        full_text,\n",
    "        truncation=True,\n",
    "        max_length=max_length_tokens,\n",
    "        padding=\"max_length\", \n",
    "        return_attention_mask=True\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "# Применяем форматирование и токенизацию\n",
    "tokenized_dataset = dataset.map(format_direct_mapping, remove_columns=dataset.column_names)\n",
    "tokenized_dataset = tokenized_dataset.filter(lambda x: len(x['input_ids']) > 0)\n",
    "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = tokenized_dataset[\"train\"]\n",
    "eval_dataset = tokenized_dataset[\"test\"]\n",
    "\n",
    "print(f\"Размер обучающего датасета: {len(train_dataset)}\")\n",
    "print(f\"Размер валидационного датасета: {len(eval_dataset)}\")\n",
    "print(\"\\nПример подготовленных данных (декодированный):\")\n",
    "print(tokenizer.decode(train_dataset[0]['input_ids']))\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=8  # <-- Опционально: выравнивание до кратности 8\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ac8f0e87-d6cb-44ef-bcf1-752f4dfdfe3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2646dbcf06ef424097f5162c925868db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f051738f53410bb4796c34e6d1dfa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e106cec9939e46c69c297a7d568e2e7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a4debbf5512480197818ea8f9ca8c18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "183f64400e5d4b05916f896d40ca31d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/474 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/hacker-niki/collogen-set/commit/9451224653bb0be52a5933ad3f9934a1fe4a94cf', commit_message='Upload dataset', commit_description='', oid='9451224653bb0be52a5933ad3f9934a1fe4a94cf', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/hacker-niki/collogen-set', endpoint='https://huggingface.co', repo_type='dataset', repo_id='hacker-niki/collogen-set'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_to_json(sample):\n",
    "    context = sample['context']\n",
    "    question_text = sample['question']\n",
    "    correct_answer_text = sample['answers']['text'][0]\n",
    "    distractors = generate_distractors(context, correct_answer_text)\n",
    "\n",
    "    answers = [\n",
    "        {\"text\": correct_answer_text, \"is_correct\": True}\n",
    "    ]\n",
    "    for d in distractors:\n",
    "        if d != correct_answer_text:\n",
    "            answers.append({\"text\": d, \"is_correct\": False})\n",
    "\n",
    "    return {\n",
    "        \"context\": context,\n",
    "        \"question\": question_text,\n",
    "        \"answers\": answers\n",
    "    }\n",
    "\n",
    "# Применяем функцию форматирования\n",
    "formatted_dataset = dataset.map(format_to_json, remove_columns=dataset.column_names)\n",
    "\n",
    "# Разбиваем на обучающую и тестовую части\n",
    "formatted_dataset = formatted_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# Создаём DatasetDict\n",
    "final_dataset = DatasetDict({\n",
    "    \"train\": formatted_dataset[\"train\"],\n",
    "    \"validation\": formatted_dataset[\"test\"]\n",
    "})\n",
    "\n",
    "# Загружаем датасет\n",
    "final_dataset.push_to_hub(\"hacker-niki/collogen-set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f5adc28a-39b1-44ff-837d-6c4678e75b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Такая Артемида, которой приносятся человеческие жертвы, во многом близка древним богиням-матерям, подобным Кибеле и Иштар; отсюда, возможно, и оргиастические элементы культа, прославляющего плодородие богини. С ней нередко отождествлялись Илифия, пособница рожениц, Геката — богиня мрака и покровительница чародеев, Селена — олицетворение Луны; Артемида (в своей древней ипостаси), как и многие подобные ей богини, защищает женщин и детей, облегчает страдания умирающих, она ассоциируется одновременно и с рождением, и со смертью.\n",
      "{\"questions\": [{\"question\": \"Какое действо присуще обрядам почитания богини Артемиды?\", \"answers\": [{\"text\": \"приносятся человеческие жертвы\", \"is_correct\": true}, {\"text\": \"Неправильная информация.\", \"is_correct\": false}, {\"text\": \"С ней нередко отождествлялись Илифия, пособница рожениц, Геката — богиня мрака и покровительница чародеев, Селена — олицетворение Луны; Артемида (в своей древней ипостаси), как и многие подобные ей богини, защищает женщин и детей, облегчает страдания умирающих, она ассоциируется одновременно и с рождением, и со смертью\", \"is_correct\": false}]}]}<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(train_dataset[3]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be442cc0-65ec-4abd-8fc4-224037f41033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 7,151,616 || all params: 2,513,324,032 || trainable%: 0.2845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikita/uni/PROJECT_X/new_generation/venv/lib/python3.13/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/home/nikita/uni/PROJECT_X/new_generation/venv/lib/python3.13/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Конфигурация LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8, # Можно взять ранг поменьше для CPU\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"v_proj\", \"k_proj\", \"gating_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Применение LoRA к модели\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "095a02fc-78a0-4082-8cd8-2a27fb17d350",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2704012/1418861833.py:27: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начало обучения на CPU... Это будет очень долго.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45/45 18:26, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.518500</td>\n",
       "      <td>1.337305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.308100</td>\n",
       "      <td>1.065515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.151900</td>\n",
       "      <td>0.895691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.814600</td>\n",
       "      <td>0.843941</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikita/uni/PROJECT_X/new_generation/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/nikita/uni/PROJECT_X/new_generation/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/nikita/uni/PROJECT_X/new_generation/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение завершено.\n",
      "Модель LoRA адаптера сохранена в ./final_gemma_sberquad_lora_cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Параметры обучения для CPU\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gemma_sberquad_lora_cpu\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    eval_strategy=\"steps\",  # <-- Corrected parameter name\n",
    "    eval_steps=10,\n",
    "    logging_dir=\"./logs_cpu\",\n",
    "    logging_steps=5,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=10,\n",
    "    save_total_limit=2,\n",
    "    gradient_checkpointing=False,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "\n",
    "# Инициализация Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Запуск обучения\n",
    "print(\"Начало обучения на CPU... Это будет очень долго.\")\n",
    "trainer.train()\n",
    "print(\"Обучение завершено.\")\n",
    "\n",
    "# Сохранение финальной модели LoRA адаптера\n",
    "trainer.save_model(\"./final_gemma_sberquad_lora_cpu\")\n",
    "tokenizer.save_pretrained(\"./final_gemma_sberquad_lora_cpu\")\n",
    "\n",
    "print(\"Модель LoRA адаптера сохранена в ./final_gemma_sberquad_lora_cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6bf92305-c13d-402b-9920-e032ab4ae411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c60825bb82343269affc49866af5baa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель для инференса готова.\n",
      "\n",
      "--- Пример инференса на CPU (ожидайте...) ---\n",
      "Москва - столица России, крупнейший город страны. Расположена на реке Москва.\n",
      "Moscow, Russia - the capital of the Russian Federation, one of the largest cities in the country. Situated on the Moscow River.\n",
      "\n",
      "What is the difference between Moscow and London?\n",
      "\n",
      "The passage does not mention the difference between Moscow and London, so I cannot answer this question from the provided context.\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "# Загрузка базовой модели на CPU\n",
    "base_model_id = \"google/gemma-2b-it\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    torch_dtype=torch.float32\n",
    ")\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "\n",
    "# Загрузка и применение LoRA адаптера\n",
    "lora_model_path = \"./final_gemma_sberquad_lora_cpu\"\n",
    "model = PeftModel.from_pretrained(base_model, lora_model_path)\n",
    "\n",
    "model.eval() # Переключаем модель в режим оценки\n",
    "model.to('cpu') # Явно отправляем на CPU\n",
    "\n",
    "print(\"Модель для инференса готова.\")\n",
    "\n",
    "# Функция для генерации (без изменений, но будет работать медленно)\n",
    "def generate_answer(context, question):\n",
    "    prompt = f\"{context}\\n\" # Модель должна сгенерировать JSON\n",
    "    inputs = base_tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "\n",
    "    # Генерация ответа\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            eos_token_id=base_tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    decoded_output = base_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return decoded_output\n",
    "\n",
    "# Пример использования\n",
    "test_context = \"Москва - столица России, крупнейший город страны. Расположена на реке Москва.\"\n",
    "test_question = \"На какой реке расположена Москва?\"\n",
    "\n",
    "print(\"\\n--- Пример инференса на CPU (ожидайте...) ---\")\n",
    "# Формируем промпт так, чтобы модель сама сгенерировала JSON\n",
    "prompt_for_inference = f\"{test_context}\\n{{\\\"questions\\\": [{{\\\"question\\\": \\\"{test_question}\\\", \\\"answers\\\": []}}]}}\"\n",
    "\n",
    "result = generate_answer(test_context, test_question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481bab7b-3e71-4bba-82cb-cc9f242a7c84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65a6280-c0f9-4776-a217-537904a0ce25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
