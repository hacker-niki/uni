{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af2858cf-f35b-4f8f-8e47-6d2ee54fe700",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU transformers peft accelerate trl datasets torch huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e245276-4776-485d-8387-2a51088e1d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = \"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\"\n",
    "login(hf_token) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac7c6609-ec05-433a-91eb-a31f3d17333b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "# Инициализируем Accelerator для CPU\n",
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "143c2d25-f14c-4edd-aa09-a4c1666d5749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "    num_rows: 45328\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Загружаем датасет SberQuAD\n",
    "dataset = load_dataset(\"sberquad\", split='train') # Используйте нужный вам сплит\n",
    "\n",
    "# (Опционально) Разделите на обучающую и валидационную выборки, если нужно\n",
    "# dataset = dataset.train_test_split(test_size=0.1)\n",
    "# train_dataset = dataset[\"train\"]\n",
    "# eval_dataset = dataset[\"test\"]\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23fba217-c530-4459-b09c-bc879a7178fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d3b141b501e4f53af24c5b9e2ab3d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# Для Phi-3 рекомендуется использовать padding_side='left' при обучении\n",
    "tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "627c155e-61bc-4f00-a21e-4128329adf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Шаг 3: Создание синтетического датасета в формате JSON\n",
    "# ==============================================================================\n",
    "import json\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Загружаем SberQuAD\n",
    "dataset = load_dataset(\"sberquad\", split='train')\n",
    "\n",
    "# Для скорости разработки возьмем небольшой срез. Для полноценного обучения используйте весь датасет.\n",
    "dataset = dataset.select(range(5000))\n",
    "\n",
    "# Функция для создания 4 неправильных ответов (дистракторов)\n",
    "def generate_distractors(context, correct_answer):\n",
    "    # Простой, но эффективный метод: берем случайные предложения из контекста,\n",
    "    # которые не содержат правильный ответ.\n",
    "    sentences = context.split('.')\n",
    "    distractors = []\n",
    "    for sent in sentences:\n",
    "        if correct_answer not in sent and len(sent.strip()) > 0:\n",
    "            distractors.append(sent.strip())\n",
    "\n",
    "    # Если предложений мало, просто добавим \"неверный ответ\"\n",
    "    while len(distractors) < 4:\n",
    "        distractors.append(\"Это заведомо неверный вариант ответа.\")\n",
    "\n",
    "    # Перемешиваем и берем 4 уникальных\n",
    "    random.shuffle(distractors)\n",
    "    return list(set(distractors))[:4]\n",
    "\n",
    "\n",
    "# Функция для форматирования данных в целевой JSON и создания промпта\n",
    "def format_data_for_training(sample):\n",
    "    context = sample['context']\n",
    "    question_text = sample['question']\n",
    "    # SberQuAD дает ответ в виде списка, берем первый\n",
    "    correct_answer_text = sample['answers']['text'][0]\n",
    "\n",
    "    # 1. Создаем 4 неправильных ответа\n",
    "    distractors = generate_distractors(context, correct_answer_text)\n",
    "\n",
    "    # 2. Формируем список ответов\n",
    "    answers = []\n",
    "    answers.append({\"text\": correct_answer_text, \"is_correct\": True})\n",
    "    for distractor in distractors:\n",
    "        answers.append({\"text\": distractor, \"is_correct\": False})\n",
    "\n",
    "    # 3. Перемешиваем варианты ответов! Это КРИТИЧЕСКИ ВАЖНО.\n",
    "    # Иначе модель выучит, что правильный ответ всегда первый.\n",
    "    random.shuffle(answers)\n",
    "\n",
    "    # 4. Собираем финальный JSON-объект\n",
    "    json_output = {\n",
    "        \"questions\": [\n",
    "            {\n",
    "                \"question\": question_text,\n",
    "                \"answers\": answers\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # 5. Создаем полный промпт для обучения\n",
    "    # Модель должна научиться по контексту генерировать JSON\n",
    "    prompt = f\"\"\"<|user|>\n",
    "Ты — интеллектуальная система, которая должна генерировать JSON-объект с вопросами и ответами строго по тексту. Каждый вопрос должен содержать пять вариантов ответов, среди которых только один правильный.\n",
    "Не добавляй никакого дополнительного текста кроме json.\n",
    "\n",
    "Контекст:\n",
    "{context}<|end|>\n",
    "<|assistant|>\n",
    "{json.dumps(json_output, ensure_ascii=False, indent=4)}<|end|>\"\"\"\n",
    "    \n",
    "    # Токенизация\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=2048, # Увеличим max_length для сложных JSON\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "# Применяем форматирование ко всему датасету\n",
    "tokenized_dataset = dataset.map(format_data_for_training)\n",
    "\n",
    "# Разделяем на train/eval\n",
    "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = tokenized_dataset[\"train\"]\n",
    "eval_dataset = tokenized_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79466209-5b13-495b-915c-1514c4fa7b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The 8-bit optimizer is not available on your device, only available on CUDA for now.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Шаг 4: Настройка LoRA-адаптера (без изменений)\n",
    "# ==============================================================================\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"qkv_proj\", \"o_proj\", \"gate_up_proj\", \"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "# (опционально) посмотреть, сколько параметров обучается\n",
    "# print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "119d9ad7-41fb-4b5a-b820-4b3583b4f389",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikita/uni/PROJECT_X/new_generation/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 10:46:27, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.346900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.297700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.340700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.292600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.289000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.218600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.264900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.229600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.252600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.199000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=1.273168420791626, metrics={'train_runtime': 39180.1474, 'train_samples_per_second': 0.01, 'train_steps_per_second': 0.003, 'total_flos': 1.84209150836736e+16, 'train_loss': 1.273168420791626, 'epoch': 0.08888888888888889})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Шаг 5: Запуск обучения (без изменений)\n",
    "# ==============================================================================\n",
    "import transformers\n",
    "\n",
    "output_dir = \"./phi3-direct-json-converter\"\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        max_steps=100,\n",
    "        learning_rate=2e-4,\n",
    "    \n",
    "        # --- ИЗМЕНЕНИЯ ДЛЯ CPU ---\n",
    "        optim=\"adamw_torch\",\n",
    "        bf16=False,\n",
    "        fp16=False,\n",
    "        # ------------------------\n",
    "    \n",
    "        logging_steps=10,\n",
    "        save_steps=50,                # <-- Оставляем, это работает в старых версиях\n",
    "        eval_steps=50,                # <-- Оставляем, это тоже работает\n",
    "        do_eval=True,                 # <-- Явно включаем оценку\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4de2a29-cf0b-4fcf-817c-980e0737f986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сохранение финальной модели...\n",
      "Модель успешно сохранена.\n"
     ]
    }
   ],
   "source": [
    "print(\"Сохранение финальной модели...\")\n",
    "trainer.save_model() \n",
    "print(\"Модель успешно сохранена.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03dc6c88-e4cd-4f4b-9427-26d1d3205d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "450e406076174dd8a6e51ea61ccdfa54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'DynamicCache' object has no attribute 'seen_tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 61\u001b[39m\n\u001b[32m     58\u001b[39m ft_model.eval()\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# Генерируем ответ\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     output_ids = \u001b[43mft_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     70\u001b[39m     \u001b[38;5;66;03m# Декодируем только сгенерированную часть\u001b[39;00m\n\u001b[32m     71\u001b[39m     response_text = tokenizer.decode(output_ids[model_input.input_ids.shape[\u001b[32m1\u001b[39m]:], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m).strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/uni/PROJECT_X/new_generation/venv/lib/python3.13/site-packages/peft/peft_model.py:1973\u001b[39m, in \u001b[36mPeftModelForCausalLM.generate\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1971\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(*args, **kwargs):\n\u001b[32m   1972\u001b[39m         kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m-> \u001b[39m\u001b[32m1973\u001b[39m         outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1974\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1975\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.base_model.generate(**kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/uni/PROJECT_X/new_generation/venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/uni/PROJECT_X/new_generation/venv/lib/python3.13/site-packages/transformers/generation/utils.py:2539\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2528\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m GenerationMixin.generate(\n\u001b[32m   2529\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2530\u001b[39m         inputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2534\u001b[39m         **kwargs,\n\u001b[32m   2535\u001b[39m     )\n\u001b[32m   2537\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.SAMPLE, GenerationMode.GREEDY_SEARCH):\n\u001b[32m   2538\u001b[39m     \u001b[38;5;66;03m# 11. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2539\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2540\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2541\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2542\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2543\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2544\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2545\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2546\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2547\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2549\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2550\u001b[39m     \u001b[38;5;66;03m# 11. run beam sample\u001b[39;00m\n\u001b[32m   2551\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._beam_search(\n\u001b[32m   2552\u001b[39m         input_ids,\n\u001b[32m   2553\u001b[39m         logits_processor=prepared_logits_processor,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2557\u001b[39m         **model_kwargs,\n\u001b[32m   2558\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/uni/PROJECT_X/new_generation/venv/lib/python3.13/site-packages/transformers/generation/utils.py:2860\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2856\u001b[39m     is_prefill = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2858\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n\u001b[32m   2859\u001b[39m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2860\u001b[39m     model_inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprepare_inputs_for_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2862\u001b[39m     \u001b[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001b[39;00m\n\u001b[32m   2863\u001b[39m     model_inputs.update({\u001b[33m\"\u001b[39m\u001b[33moutput_attentions\u001b[39m\u001b[33m\"\u001b[39m: output_attentions} \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;28;01melse\u001b[39;00m {})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-128k-instruct/072cb7562cb8c4adf682a8e186aaafa49469eb5d/modeling_phi3.py:1291\u001b[39m, in \u001b[36mPhi3ForCausalLM.prepare_inputs_for_generation\u001b[39m\u001b[34m(self, input_ids, past_key_values, attention_mask, inputs_embeds, **kwargs)\u001b[39m\n\u001b[32m   1289\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(past_key_values, Cache):\n\u001b[32m   1290\u001b[39m     cache_length = past_key_values.get_seq_length()\n\u001b[32m-> \u001b[39m\u001b[32m1291\u001b[39m     past_length = \u001b[43mpast_key_values\u001b[49m\u001b[43m.\u001b[49m\u001b[43mseen_tokens\u001b[49m\n\u001b[32m   1292\u001b[39m     max_cache_length = past_key_values.get_max_length()\n\u001b[32m   1293\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mAttributeError\u001b[39m: 'DynamicCache' object has no attribute 'seen_tokens'"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Шаг 6: Проверка работы новой модели-конвертера\n",
    "# ==============================================================================\n",
    "# Рекомендуется перезапустить ядро для освобождения VRAM\n",
    "\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import json\n",
    "\n",
    "# --- Загружаем базовую модель ---\n",
    "base_model_id = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_use_double_quant=True, \n",
    "    bnb_4bit_quant_type=\"nf4\", \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id, \n",
    "    quantization_config=bnb_config, \n",
    "    trust_remote_code=True, \n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)\n",
    "tokenizer.padding_side = 'left'\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "# --- Загружаем наш LoRA-адаптер из корневой папки вывода ---\n",
    "adapter_path = \"./phi3-direct-json-converter\" \n",
    "ft_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "# --- Запускаем инференс ---\n",
    "test_context = \"\"\"В 1831 году Майкл Фарадей открыл электромагнитную индукцию — явление возникновения электрического тока в замкнутом контуре при изменении магнитного потока, проходящего через него. Это открытие легло в основу работы большинства современных электрогенераторов и трансформаторов.\"\"\"\n",
    "\n",
    "eval_prompt = f\"\"\"<|user|>\n",
    "Ты — интеллектуальная система, которая должна генерировать JSON-объект с вопросами и ответами строго по тексту. Каждый вопрос должен содержать пять вариантов ответов, среди которых только один правильный.\n",
    "Не добавляй никакого дополнительного текста кроме json.\n",
    "\n",
    "Контекст:\n",
    "{test_context}<|end|>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    # Генерируем ответ\n",
    "    output_ids = ft_model.generate(\n",
    "        **model_input,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "    )[0]\n",
    "\n",
    "    # Декодируем только сгенерированную часть\n",
    "    response_text = tokenizer.decode(output_ids[model_input.input_ids.shape[1]:], skip_special_tokens=True).strip()\n",
    "    \n",
    "    print(\"--- Сырой вывод модели ---\")\n",
    "    print(response_text)\n",
    "    \n",
    "    print(\"\\n--- Проверка валидности JSON ---\")\n",
    "    try:\n",
    "        json_part = response_text[response_text.find('{'):response_text.rfind('}')+1]\n",
    "        parsed_json = json.loads(json_part)\n",
    "        print(\"JSON валиден!\")\n",
    "        print(json.dumps(parsed_json, indent=4, ensure_ascii=False))\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Ошибка: Модель сгенерировала невалидный JSON. {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Произошла неожиданная ошибка при парсинге: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561970c1-ffdd-4fa3-b4e6-2a50719840f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
